{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "91c998a5",
      "metadata": {
        "id": "91c998a5"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/examples/agent/multi_document_agents-v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43497beb-817d-4366-9156-f4d7f0d44942",
      "metadata": {
        "id": "43497beb-817d-4366-9156-f4d7f0d44942"
      },
      "source": [
        "# Multi-Document Agents (V1)\n",
        "\n",
        "In this guide, you learn towards setting up a multi-document agent over the LlamaIndex documentation.\n",
        "\n",
        "This is an extension of V0 multi-document agents with the additional features:\n",
        "- Reranking during document (tool) retrieval\n",
        "- Query planning tool that the agent can use to plan\n",
        "\n",
        "\n",
        "We do this with the following architecture:\n",
        "\n",
        "- setup a \"document agent\" over each Document: each doc agent can do QA/summarization within its doc\n",
        "- setup a top-level agent over this set of document agents. Do tool retrieval and then do CoT over the set of tools to answer a question."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77ac7184",
      "metadata": {
        "id": "77ac7184"
      },
      "source": [
        "If you're opening this Notebook on colab, you will probably need to install LlamaIndex ðŸ¦™."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "a3mTHihhKgtg",
        "outputId": "4de0deb0-1e9f-408d-b45d-cc1050041262",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "a3mTHihhKgtg",
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd '/content/drive/My Drive/Code Documentation Project/Issue_Report/'"
      ],
      "metadata": {
        "id": "mE3TthF8KkTA",
        "outputId": "548161a2-ed24-4220-b479-96cde572c3f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "mE3TthF8KkTA",
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/Code Documentation Project/Issue_Report\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "4eff88ab",
      "metadata": {
        "id": "4eff88ab",
        "outputId": "09c94479-c5b9-4e96-df30-1c841278a41e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing collected packages: urllib3, mypy-extensions, marshmallow, jsonpointer, h11, deprecated, aiostream, typing-inspect, jsonpatch, httpcore, tiktoken, langsmith, httpx, dataclasses-json, openai, langchain, llama-index\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.0.7\n",
            "    Uninstalling urllib3-2.0.7:\n",
            "      Successfully uninstalled urllib3-2.0.7\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiostream-0.5.2 dataclasses-json-0.5.14 deprecated-1.2.14 h11-0.14.0 httpcore-1.0.2 httpx-0.25.1 jsonpatch-1.33 jsonpointer-2.4 langchain-0.0.335 langsmith-0.0.64 llama-index-0.8.69.post2 marshmallow-3.20.1 mypy-extensions-1.0.0 openai-1.2.4 tiktoken-0.5.1 typing-inspect-0.9.0 urllib3-1.26.18\n"
          ]
        }
      ],
      "source": [
        "!pip install llama-index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "1f0e47ac-ec6d-48eb-93a3-0e1fcab22112",
      "metadata": {
        "id": "1f0e47ac-ec6d-48eb-93a3-0e1fcab22112"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9be00aba-b6c5-4940-9825-81c5d2cd2f0b",
      "metadata": {
        "id": "9be00aba-b6c5-4940-9825-81c5d2cd2f0b"
      },
      "source": [
        "## Setup and Download Data\n",
        "\n",
        "In this section, we'll load in the LlamaIndex documentation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49893d69-c106-4169-92c3-6b5b751066e9",
      "metadata": {
        "id": "49893d69-c106-4169-92c3-6b5b751066e9"
      },
      "outputs": [],
      "source": [
        "domain = \"docs.llamaindex.ai\"\n",
        "docs_url = \"https://docs.llamaindex.ai/en/latest/\"\n",
        "!wget -e robots=off --recursive --no-clobber --page-requisites --html-extension --convert-links --restrict-file-names=windows --domains {domain} --no-parent {docs_url}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama_hub"
      ],
      "metadata": {
        "id": "2RAikplg7Dcg",
        "outputId": "27a7db0b-b0a0-45a7-d207-4e38d7b2ed42",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "2RAikplg7Dcg",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing collected packages: retrying, html2text, atlassian-python-api, llama_hub\n",
            "Successfully installed atlassian-python-api-3.41.3 html2text-2020.1.16 llama_hub-0.0.44 retrying-1.3.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "c661cb62-1e18-410c-bc2e-e707b66596a3",
      "metadata": {
        "id": "c661cb62-1e18-410c-bc2e-e707b66596a3"
      },
      "outputs": [],
      "source": [
        "from llama_hub.file.unstructured.base import UnstructuredReader\n",
        "from pathlib import Path\n",
        "from llama_index.llms import OpenAI\n",
        "from llama_index import ServiceContext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "dd2452e8-061b-45b0-990d-36aa39ae02a4",
      "metadata": {
        "id": "dd2452e8-061b-45b0-990d-36aa39ae02a4",
        "outputId": "48e671cc-ea9d-477b-e766-4e4c69f1c92f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ],
      "source": [
        "reader = UnstructuredReader()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_files_gen = Path(\"./issue1/\").rglob(\"*\")\n",
        "all_files = [f.resolve() for f in all_files_gen]\n",
        "len(all_files)"
      ],
      "metadata": {
        "id": "-F4y0IUZKvt5",
        "outputId": "9da3f5ba-23c6-4783-ef52-66eb537ed088",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "-F4y0IUZKvt5",
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "44feebd5-0430-4d73-9cb1-a3de73c1f13e",
      "metadata": {
        "id": "44feebd5-0430-4d73-9cb1-a3de73c1f13e"
      },
      "outputs": [],
      "source": [
        "all_files_gen = Path(\"./docs.llamaindex.ai/\").rglob(\"*\")\n",
        "all_files = [f.resolve() for f in all_files_gen]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "3d837b4b-130c-493c-b62e-6662904c20ca",
      "metadata": {
        "id": "3d837b4b-130c-493c-b62e-6662904c20ca"
      },
      "outputs": [],
      "source": [
        "all_html_files = [f for f in all_files if f.suffix.lower() == \".html\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "3cddf0f5-3c5f-4d42-868d-54bedb12d02b",
      "metadata": {
        "id": "3cddf0f5-3c5f-4d42-868d-54bedb12d02b",
        "outputId": "148812a9-0b50-43a6-f6cf-278522159960",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "478"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "len(all_html_files)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unstructured"
      ],
      "metadata": {
        "id": "7v6hkXuR7P_G"
      },
      "id": "7v6hkXuR7P_G",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index import Document\n",
        "import random\n",
        "exceptions = ['segmentation error', 'assertion error', 'null pointer exception']\n",
        "# TODO: set to higher value if you want more docs\n",
        "doc_limit = 100\n",
        "\n",
        "docs = []\n",
        "for idx, f in enumerate(all_files):\n",
        "    if idx > doc_limit:\n",
        "        break\n",
        "    print(f\"Idx {idx}/{len(all_files)}\")\n",
        "    loaded_docs = reader.load_data(file=f, split_documents=True)\n",
        "    # Hardcoded Index. Everything before this is ToC for all pages\n",
        "    start_idx = 72\n",
        "\n",
        "    exception = random.choice(exceptions)\n",
        "    loaded_doc = Document(\n",
        "        text=\"\\n\\n\".join([d.get_content() for d in loaded_docs[72:]]),\n",
        "        metadata={\"path\": str(f),\"issue_number\": str(os.path.basename(f)).split('.')[0],'Exception Type':exception},\n",
        "\n",
        "    )\n",
        "    print(loaded_doc.metadata)\n",
        "    docs.append(loaded_doc)"
      ],
      "metadata": {
        "id": "BPo7AffFLVxJ",
        "outputId": "c7c500a7-eee1-46ba-83f9-4d93e89e6720",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "BPo7AffFLVxJ",
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Idx 0/15\n",
            "{'path': '/content/drive/MyDrive/Code Documentation Project/Issue_Report/issue1/18305.txt', 'issue_number': '18305', 'Exception Type': 'assertion error'}\n",
            "Idx 1/15\n",
            "{'path': '/content/drive/MyDrive/Code Documentation Project/Issue_Report/issue1/18306.txt', 'issue_number': '18306', 'Exception Type': 'segmentation error'}\n",
            "Idx 2/15\n",
            "{'path': '/content/drive/MyDrive/Code Documentation Project/Issue_Report/issue1/18307.txt', 'issue_number': '18307', 'Exception Type': 'null pointer exception'}\n",
            "Idx 3/15\n",
            "{'path': '/content/drive/MyDrive/Code Documentation Project/Issue_Report/issue1/18308.txt', 'issue_number': '18308', 'Exception Type': 'assertion error'}\n",
            "Idx 4/15\n",
            "{'path': '/content/drive/MyDrive/Code Documentation Project/Issue_Report/issue1/18309.txt', 'issue_number': '18309', 'Exception Type': 'assertion error'}\n",
            "Idx 5/15\n",
            "{'path': '/content/drive/MyDrive/Code Documentation Project/Issue_Report/issue1/18316.txt', 'issue_number': '18316', 'Exception Type': 'null pointer exception'}\n",
            "Idx 6/15\n",
            "{'path': '/content/drive/MyDrive/Code Documentation Project/Issue_Report/issue1/18318.txt', 'issue_number': '18318', 'Exception Type': 'assertion error'}\n",
            "Idx 7/15\n",
            "{'path': '/content/drive/MyDrive/Code Documentation Project/Issue_Report/issue1/18320.txt', 'issue_number': '18320', 'Exception Type': 'segmentation error'}\n",
            "Idx 8/15\n",
            "{'path': '/content/drive/MyDrive/Code Documentation Project/Issue_Report/issue1/18321.txt', 'issue_number': '18321', 'Exception Type': 'segmentation error'}\n",
            "Idx 9/15\n",
            "{'path': '/content/drive/MyDrive/Code Documentation Project/Issue_Report/issue1/18322.txt', 'issue_number': '18322', 'Exception Type': 'null pointer exception'}\n",
            "Idx 10/15\n",
            "{'path': '/content/drive/MyDrive/Code Documentation Project/Issue_Report/issue1/18326.txt', 'issue_number': '18326', 'Exception Type': 'null pointer exception'}\n",
            "Idx 11/15\n",
            "{'path': '/content/drive/MyDrive/Code Documentation Project/Issue_Report/issue1/18328.txt', 'issue_number': '18328', 'Exception Type': 'assertion error'}\n",
            "Idx 12/15\n",
            "{'path': '/content/drive/MyDrive/Code Documentation Project/Issue_Report/issue1/18329.txt', 'issue_number': '18329', 'Exception Type': 'assertion error'}\n",
            "Idx 13/15\n",
            "{'path': '/content/drive/MyDrive/Code Documentation Project/Issue_Report/issue1/18324.txt', 'issue_number': '18324', 'Exception Type': 'assertion error'}\n",
            "Idx 14/15\n",
            "{'path': '/content/drive/MyDrive/Code Documentation Project/Issue_Report/issue1/18304.txt', 'issue_number': '18304', 'Exception Type': 'segmentation error'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a1dd0cf-5da2-4ac0-bfd1-8f48921518c5",
      "metadata": {
        "id": "1a1dd0cf-5da2-4ac0-bfd1-8f48921518c5"
      },
      "outputs": [],
      "source": [
        "from llama_index import Document\n",
        "\n",
        "# TODO: set to higher value if you want more docs\n",
        "doc_limit = 100\n",
        "\n",
        "docs = []\n",
        "for idx, f in enumerate(all_html_files):\n",
        "    if idx > doc_limit:\n",
        "        break\n",
        "    print(f\"Idx {idx}/{len(all_html_files)}\")\n",
        "    loaded_docs = reader.load_data(file=f, split_documents=True)\n",
        "    # Hardcoded Index. Everything before this is ToC for all pages\n",
        "    start_idx = 72\n",
        "    loaded_doc = Document(\n",
        "        text=\"\\n\\n\".join([d.get_content() for d in loaded_docs[72:]]),\n",
        "        metadata={\"path\": str(f)},\n",
        "    )\n",
        "    print(loaded_doc.metadata[\"path\"])\n",
        "    docs.append(loaded_doc)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6189aaf4-2eb7-40bc-9e83-79ce4f221b4b",
      "metadata": {
        "id": "6189aaf4-2eb7-40bc-9e83-79ce4f221b4b"
      },
      "source": [
        "Define LLM + Service Context + Callback Manager"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "dd6e5e48-91b9-4701-a85d-d98c92323350",
      "metadata": {
        "id": "dd6e5e48-91b9-4701-a85d-d98c92323350"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
        "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
        "llm = OpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n",
        "service_context = ServiceContext.from_defaults(llm=llm)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4eeef31a-fc25-4367-a5ba-945f81d04cf9",
      "metadata": {
        "id": "4eeef31a-fc25-4367-a5ba-945f81d04cf9"
      },
      "source": [
        "## Building Multi-Document Agents\n",
        "\n",
        "In this section we show you how to construct the multi-document agent. We first build a document agent for each document, and then define the top-level parent agent with an object index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "c9c3c2a9-c546-410d-9fbd-1a76f8da4ecc",
      "metadata": {
        "id": "c9c3c2a9-c546-410d-9fbd-1a76f8da4ecc"
      },
      "outputs": [],
      "source": [
        "from llama_index import VectorStoreIndex, SummaryIndex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "e2a26f16-cfe9-4221-b169-57df91b197da",
      "metadata": {
        "id": "e2a26f16-cfe9-4221-b169-57df91b197da"
      },
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "976cd798-2e8d-474c-922a-51b12c5c6f36",
      "metadata": {
        "id": "976cd798-2e8d-474c-922a-51b12c5c6f36"
      },
      "source": [
        "### Build Document Agent for each Document\n",
        "\n",
        "In this section we define \"document agents\" for each document.\n",
        "\n",
        "We define both a vector index (for semantic search) and summary index (for summarization) for each document. The two query engines are then converted into tools that are passed to an OpenAI function calling agent.\n",
        "\n",
        "This document agent can dynamically choose to perform semantic search or summarization within a given document.\n",
        "\n",
        "We create a separate document agent for each city."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "id": "eacdf3a7-cfe3-4c2b-9037-b28a065ed148",
      "metadata": {
        "id": "eacdf3a7-cfe3-4c2b-9037-b28a065ed148"
      },
      "outputs": [],
      "source": [
        "from llama_index.agent import OpenAIAgent\n",
        "from llama_index import load_index_from_storage, StorageContext\n",
        "from llama_index.tools import QueryEngineTool, ToolMetadata\n",
        "from llama_index.node_parser import SimpleNodeParser\n",
        "import os\n",
        "from tqdm.notebook import tqdm\n",
        "import pickle\n",
        "\n",
        "\n",
        "async def build_agent_per_doc(nodes, file_base):\n",
        "    print(file_base)\n",
        "\n",
        "    vi_out_path = f\"./data/issue1/{file_base}\"\n",
        "    summary_out_path = f\"./data/issue1/{file_base}_summary.pkl\"\n",
        "    if not os.path.exists(vi_out_path):\n",
        "        Path(\"./data/issue1/\").mkdir(parents=True, exist_ok=True)\n",
        "        # build vector index\n",
        "        vector_index = VectorStoreIndex(nodes, service_context=service_context)\n",
        "        vector_index.storage_context.persist(persist_dir=vi_out_path)\n",
        "    else:\n",
        "        vector_index = load_index_from_storage(\n",
        "            StorageContext.from_defaults(persist_dir=vi_out_path),\n",
        "            service_context=service_context,\n",
        "        )\n",
        "\n",
        "    # build summary index\n",
        "    summary_index = SummaryIndex(nodes, service_context=service_context)\n",
        "\n",
        "    # define query engines\n",
        "    vector_query_engine = vector_index.as_query_engine()\n",
        "    summary_query_engine = summary_index.as_query_engine(\n",
        "        response_mode=\"tree_summarize\"\n",
        "    )\n",
        "\n",
        "    # extract a summary\n",
        "    if not os.path.exists(summary_out_path):\n",
        "        Path(summary_out_path).parent.mkdir(parents=True, exist_ok=True)\n",
        "        summary = str(\n",
        "            await summary_query_engine.aquery(\n",
        "                \"Extract a concise 1-2 line summary of this document\"\n",
        "            )\n",
        "        )\n",
        "        pickle.dump(summary, open(summary_out_path, \"wb\"))\n",
        "    else:\n",
        "        summary = pickle.load(open(summary_out_path, \"rb\"))\n",
        "\n",
        "    # define tools\n",
        "    query_engine_tools = [\n",
        "        QueryEngineTool(\n",
        "            query_engine=vector_query_engine,\n",
        "            metadata=ToolMetadata(\n",
        "                name=f\"vector_tool_{file_base}\",\n",
        "                description=f\"Useful for questions related to specific facts\",\n",
        "            ),\n",
        "        ),\n",
        "        QueryEngineTool(\n",
        "            query_engine=summary_query_engine,\n",
        "            metadata=ToolMetadata(\n",
        "                name=f\"summary_tool_{file_base}\",\n",
        "                description=f\"Useful for summarization questions\",\n",
        "            ),\n",
        "        ),\n",
        "    ]\n",
        "\n",
        "    # build agent\n",
        "    function_llm = OpenAI(model=\"gpt-4\")\n",
        "    agent = OpenAIAgent.from_tools(\n",
        "        query_engine_tools,\n",
        "        llm=function_llm,\n",
        "        verbose=True,\n",
        "        system_prompt=f\"\"\"\\\n",
        "You are a specialized agent designed to answer queries about the `{file_base}.txt` part of issue reports.\n",
        "You must ALWAYS use at least one of the tools provided when answering a question; do NOT rely on prior knowledge.\\\n",
        "\"\"\",\n",
        "    )\n",
        "\n",
        "    return agent, summary\n",
        "\n",
        "\n",
        "async def build_agents(docs):\n",
        "    node_parser = SimpleNodeParser.from_defaults()\n",
        "\n",
        "    # Build agents dictionary\n",
        "    agents_dict = {}\n",
        "    extra_info_dict = {}\n",
        "\n",
        "    # # this is for the baseline\n",
        "    # all_nodes = []\n",
        "\n",
        "    for idx, doc in enumerate(tqdm(docs)):\n",
        "        nodes = node_parser.get_nodes_from_documents([doc])\n",
        "        # all_nodes.extend(nodes)\n",
        "\n",
        "        # ID will be base + parent\n",
        "        file_path = Path(doc.metadata[\"path\"])\n",
        "        file_base = str(file_path.parent.stem) + \"_\" + str(file_path.stem)\n",
        "        agent, summary = await build_agent_per_doc(nodes, file_base)\n",
        "\n",
        "        agents_dict[file_base] = agent\n",
        "        extra_info_dict[file_base] = {\"summary\": summary, \"nodes\": nodes}\n",
        "\n",
        "    return agents_dict, extra_info_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "id": "44748b46-dd6b-4d4f-bc70-7022ae96413f",
      "metadata": {
        "id": "44748b46-dd6b-4d4f-bc70-7022ae96413f",
        "outputId": "fd24c593-d271-474f-9ff7-28bb2620d4f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317,
          "referenced_widgets": [
            "e165d21342764c27be24919690efb684",
            "148af4a51c964e12b954e874020ce652",
            "c35ef3b257904bbd81cd1b72b637738f",
            "c2db72fed34a4cf4a57b242c9a34a738",
            "330c23ea42e54e96b795c7358f0df652",
            "39e1da326e4743369d6d07ffb840b1e7",
            "ad576ad0749e4606bc64fa4547751240",
            "689225f033f841aab98dc6f9ac61ec2d",
            "3eb88aed177e43de9b7d5c965713e0bf",
            "e59dbb0078eb4f4098d583e1bfa0a79d",
            "b84a3bdf4bb34351b7f4048c33f09307"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/15 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e165d21342764c27be24919690efb684"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "issue1_18305\n",
            "issue1_18306\n",
            "issue1_18307\n",
            "issue1_18308\n",
            "issue1_18309\n",
            "issue1_18316\n",
            "issue1_18318\n",
            "issue1_18320\n",
            "issue1_18321\n",
            "issue1_18322\n",
            "issue1_18326\n",
            "issue1_18328\n",
            "issue1_18329\n",
            "issue1_18324\n",
            "issue1_18304\n"
          ]
        }
      ],
      "source": [
        "agents_dict, extra_info_dict = await build_agents(docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "899ca55b-0c02-429b-a765-8e4f806d503f",
      "metadata": {
        "id": "899ca55b-0c02-429b-a765-8e4f806d503f"
      },
      "source": [
        "### Build Retriever-Enabled OpenAI Agent\n",
        "\n",
        "We build a top-level agent that can orchestrate across the different document agents to answer any user query.\n",
        "\n",
        "This `RetrieverOpenAIAgent` performs tool retrieval before tool use (unlike a default agent that tries to put all tools in the prompt).\n",
        "\n",
        "**Improvements from V0**: We make the following improvements compared to the \"base\" version in V0.\n",
        "\n",
        "- Adding in reranking: we use Cohere reranker to better filter the candidate set of documents.\n",
        "- Adding in a query planning tool: we add an explicit query planning tool that's dynamically created based on the set of retrieved tools.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "id": "6884ff15-bf40-4bdd-a1e3-58cbd056a12a",
      "metadata": {
        "id": "6884ff15-bf40-4bdd-a1e3-58cbd056a12a"
      },
      "outputs": [],
      "source": [
        "# define tool for each document agent\n",
        "all_tools = []\n",
        "for file_base, agent in agents_dict.items():\n",
        "    summary = extra_info_dict[file_base][\"summary\"]\n",
        "    doc_tool = QueryEngineTool(\n",
        "        query_engine=agent,\n",
        "        metadata=ToolMetadata(\n",
        "            name=f\"tool_{file_base}\",\n",
        "            description=summary,\n",
        "        ),\n",
        "    )\n",
        "    all_tools.append(doc_tool)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "id": "346ed0e1-b96f-446b-a768-4f11a9a1a7f6",
      "metadata": {
        "id": "346ed0e1-b96f-446b-a768-4f11a9a1a7f6",
        "outputId": "f46be67e-951e-4e45-d6d8-7168b9b61924",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ToolMetadata(description='Empty Response', name='tool_issue1_18305', fn_schema=<class 'llama_index.tools.types.DefaultToolFnSchema'>)\n"
          ]
        }
      ],
      "source": [
        "print(all_tools[0].metadata)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "id": "b266ad43-c3fd-41cb-9e3b-4cb2bb2c2e5f",
      "metadata": {
        "id": "b266ad43-c3fd-41cb-9e3b-4cb2bb2c2e5f"
      },
      "outputs": [],
      "source": [
        "# define an \"object\" index and retriever over these tools\n",
        "from llama_index import VectorStoreIndex\n",
        "from llama_index.objects import (\n",
        "    ObjectIndex,\n",
        "    SimpleToolNodeMapping,\n",
        "    ObjectRetriever,\n",
        ")\n",
        "from llama_index.retrievers import BaseRetriever\n",
        "from llama_index.indices.postprocessor import CohereRerank\n",
        "from llama_index.tools import QueryPlanTool\n",
        "from llama_index.query_engine import SubQuestionQueryEngine\n",
        "from llama_index.llms import OpenAI\n",
        "\n",
        "llm = OpenAI(model_name=\"gpt-4-0613\")\n",
        "\n",
        "tool_mapping = SimpleToolNodeMapping.from_objects(all_tools)\n",
        "obj_index = ObjectIndex.from_objects(\n",
        "    all_tools,\n",
        "    tool_mapping,\n",
        "    VectorStoreIndex,\n",
        ")\n",
        "vector_node_retriever = obj_index.as_node_retriever(similarity_top_k=10)\n",
        "\n",
        "\n",
        "# define a custom retriever with reranking\n",
        "class CustomRetriever(BaseRetriever):\n",
        "    def __init__(self, vector_retriever, postprocessor=None):\n",
        "        self._vector_retriever = vector_retriever\n",
        "        self._postprocessor = postprocessor or CohereRerank(top_n=5)\n",
        "\n",
        "    def _retrieve(self, query_bundle):\n",
        "        retrieved_nodes = self._vector_retriever.retrieve(query_bundle)\n",
        "        filtered_nodes = self._postprocessor.postprocess_nodes(\n",
        "            retrieved_nodes, query_bundle=query_bundle\n",
        "        )\n",
        "\n",
        "        return filtered_nodes\n",
        "\n",
        "\n",
        "# define a custom object retriever that adds in a query planning tool\n",
        "class CustomObjectRetriever(ObjectRetriever):\n",
        "    def __init__(self, retriever, object_node_mapping, all_tools, llm=None):\n",
        "        self._retriever = retriever\n",
        "        self._object_node_mapping = object_node_mapping\n",
        "        self._llm = llm or OpenAI(\"gpt-4-0613\")\n",
        "\n",
        "    def retrieve(self, query_bundle):\n",
        "        nodes = self._retriever.retrieve(query_bundle)\n",
        "        tools = [self._object_node_mapping.from_node(n.node) for n in nodes]\n",
        "\n",
        "        sub_question_sc = ServiceContext.from_defaults(llm=self._llm)\n",
        "        sub_question_engine = SubQuestionQueryEngine.from_defaults(\n",
        "            query_engine_tools=tools, service_context=sub_question_sc\n",
        "        )\n",
        "        sub_question_description = f\"\"\"\\\n",
        "Useful for any queries that involve comparing multiple documents. ALWAYS use this tool for comparison queries - make sure to call this \\\n",
        "tool with the original query. Do NOT use the other tools for any queries involving multiple documents.\n",
        "\"\"\"\n",
        "        sub_question_tool = QueryEngineTool(\n",
        "            query_engine=sub_question_engine,\n",
        "            metadata=ToolMetadata(\n",
        "                name=\"compare_tool\", description=sub_question_description\n",
        "            ),\n",
        "        )\n",
        "\n",
        "        return tools + [sub_question_tool]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['COHERE_API_KEY'] = 'VJaf75J148x8fhvBGz7cPs9c3Ujmq2xFnUdUibW8'"
      ],
      "metadata": {
        "id": "lF4NLOai-scY"
      },
      "id": "lF4NLOai-scY",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install cohere"
      ],
      "metadata": {
        "id": "qG9DQEG7-7-t",
        "outputId": "3a610e8d-bcd8-4b6f-c3cc-c3927c69b698",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "qG9DQEG7-7-t",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing collected packages: fastavro, cohere\n",
            "Successfully installed cohere-4.34 fastavro-1.8.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "id": "0ba0d1a6-e324-4faa-b72b-d340904e65b2",
      "metadata": {
        "id": "0ba0d1a6-e324-4faa-b72b-d340904e65b2"
      },
      "outputs": [],
      "source": [
        "custom_node_retriever = CustomRetriever(vector_node_retriever)\n",
        "\n",
        "# wrap it with ObjectRetriever to return objects\n",
        "custom_obj_retriever = CustomObjectRetriever(\n",
        "    custom_node_retriever, tool_mapping, all_tools, llm=llm\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "id": "8654ce2a-cce7-44fc-8445-8bbcfdf7ee91",
      "metadata": {
        "id": "8654ce2a-cce7-44fc-8445-8bbcfdf7ee91",
        "outputId": "4dcff612-8f85-4678-8be9-06eeaa6164e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6\n"
          ]
        }
      ],
      "source": [
        "tmps = custom_obj_retriever.retrieve(\"hello\")\n",
        "print(len(tmps))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "id": "fed38942-1e37-4c61-89fa-d2ef41151831",
      "metadata": {
        "id": "fed38942-1e37-4c61-89fa-d2ef41151831"
      },
      "outputs": [],
      "source": [
        "from llama_index.agent import FnRetrieverOpenAIAgent, ReActAgent\n",
        "\n",
        "top_agent = FnRetrieverOpenAIAgent.from_retriever(\n",
        "    custom_obj_retriever,\n",
        "    system_prompt=\"\"\" \\\n",
        "You are an agent designed to answer queries about the documentation.\n",
        "Please always use the tools provided to answer a question. Do not rely on prior knowledge.\\\n",
        "\n",
        "\"\"\",\n",
        "    llm=llm,\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "# top_agent = ReActAgent.from_tools(\n",
        "#     tool_retriever=custom_obj_retriever,\n",
        "#     system_prompt=\"\"\" \\\n",
        "# You are an agent designed to answer queries about the documentation.\n",
        "# Please always use the tools provided to answer a question. Do not rely on prior knowledge.\\\n",
        "\n",
        "# \"\"\",\n",
        "#     llm=llm,\n",
        "#     verbose=True,\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa32b97c-6779-4b60-823d-6ca3be6f358a",
      "metadata": {
        "id": "aa32b97c-6779-4b60-823d-6ca3be6f358a"
      },
      "source": [
        "### Define Baseline Vector Store Index\n",
        "\n",
        "As a point of comparison, we define a \"naive\" RAG pipeline which dumps all docs into a single vector index collection.\n",
        "\n",
        "We set the top_k = 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "id": "f2f54834-1597-46ce-b0d3-0456bfa0d368",
      "metadata": {
        "id": "f2f54834-1597-46ce-b0d3-0456bfa0d368"
      },
      "outputs": [],
      "source": [
        "all_nodes = [\n",
        "    n for extra_info in extra_info_dict.values() for n in extra_info[\"nodes\"]\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_nodes[0]"
      ],
      "metadata": {
        "id": "-u12vPuNNP1B"
      },
      "id": "-u12vPuNNP1B",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "id": "60dfc88f-6f47-4ef2-9ae6-74abde06a485",
      "metadata": {
        "id": "60dfc88f-6f47-4ef2-9ae6-74abde06a485"
      },
      "outputs": [],
      "source": [
        "base_index = VectorStoreIndex(all_nodes)\n",
        "base_query_engine = base_index.as_query_engine(similarity_top_k=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8dedb927-a992-4f21-a0fb-4ce4361adcb3",
      "metadata": {
        "id": "8dedb927-a992-4f21-a0fb-4ce4361adcb3"
      },
      "source": [
        "## Running Example Queries\n",
        "\n",
        "Let's run some example queries, ranging from QA / summaries over a single document to QA / summarization over multiple documents."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = base_query_engine.query(\n",
        "    \"List out all performance-related issues\")\n",
        "print(str(response))"
      ],
      "metadata": {
        "id": "FTMUn6ajMtis",
        "outputId": "0aa9293e-5386-4888-af73-1b4cd6f03f5d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "FTMUn6ajMtis",
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are no specific performance-related issues mentioned in the given context information.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "8e743c62-7dd8-4ac9-85a5-f1cbc112a79c",
      "metadata": {
        "id": "8e743c62-7dd8-4ac9-85a5-f1cbc112a79c"
      },
      "outputs": [],
      "source": [
        "response = top_agent.query(\n",
        "    \"Tell me about the different types of evaluation in LlamaIndex\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "a4ce2a76-5779-4acf-9337-69109dae7fd6",
      "metadata": {
        "id": "a4ce2a76-5779-4acf-9337-69109dae7fd6",
        "outputId": "4f22c7de-840a-4eed-e152-2f4b09130137",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are two types of evaluation in LlamaIndex:\n",
            "\n",
            "1. LLM-based evaluation: This type of evaluation is facilitated by the BaseEvaluator class. It involves running evaluations using query strings, retrieved contexts, and generated response strings. The BaseEvaluator class provides methods for managing prompts and customizing the evaluation logic.\n",
            "\n",
            "2. Retrieval-based evaluation: This type of evaluation is facilitated by the BaseRetrievalEvaluator class. It involves evaluating the performance of retrievers using metrics. The BaseRetrievalEvaluator class includes a field for specifying the metrics to be evaluated.\n",
            "\n",
            "Both types of evaluation allow for customization and can be used to assess the performance of different components in the LlamaIndex framework.\n"
          ]
        }
      ],
      "source": [
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "id": "af28b422-fb73-4b59-9e77-3ba3afa87795",
      "metadata": {
        "id": "af28b422-fb73-4b59-9e77-3ba3afa87795",
        "outputId": "0e2cc716-1d63-4594-b98d-2619ad7fb9cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There is no information provided in the given context about LlamaIndex or any types of evaluation related to it.\n"
          ]
        }
      ],
      "source": [
        "# baseline\n",
        "response = base_query_engine.query(\n",
        "    \"Tell me about the different types of evaluation in LlamaIndex\"\n",
        ")\n",
        "print(str(response))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = base_query_engine.query(\n",
        "    \"Can you compare the tree index and list index at a very high-level?\"\n",
        ")\n",
        "print(str(response))"
      ],
      "metadata": {
        "id": "6ajhscKu_m4A",
        "outputId": "b4aab206-f1f3-401d-d1db-1fd626e52197",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "6ajhscKu_m4A",
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Yes, at a very high-level, the tree index and list index are both used to organize and access data. However, they have different structures and characteristics. A tree index is typically used in hierarchical data structures and allows for efficient searching, insertion, and deletion operations. It organizes data in a tree-like structure with nodes and branches. On the other hand, a list index is used in linear data structures and provides sequential access to elements. It stores data in a linear sequence and allows for efficient traversal and indexing operations. Overall, while both the tree index and list index serve the purpose of organizing and accessing data, they have different structures and are suited for different types of data and operations.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "id": "ee6ef20c-3ccc-46c3-ad87-667138d78d5d",
      "metadata": {
        "id": "ee6ef20c-3ccc-46c3-ad87-667138d78d5d",
        "outputId": "04d7c3e6-cbf3-40b4-94d7-b13f0b677364",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "STARTING TURN 1\n",
            "---------------\n",
            "\n",
            "=== Calling Function ===\n",
            "Calling function: compare_tool with args: {\n",
            "  \"input\": \"content in the contributions page vs. index page\"\n",
            "}\n",
            "Generated 2 sub questions.\n",
            "\u001b[1;3;38;2;237;90;200m[tool_issue1_18308] Q: What is the content in the contributions page?\n",
            "\u001b[0mSTARTING TURN 1\n",
            "---------------\n",
            "\n",
            "\u001b[1;3;38;2;90;149;237m[tool_issue1_18308] Q: What is the content in the index page?\n",
            "\u001b[0mSTARTING TURN 1\n",
            "---------------\n",
            "\n",
            "=== Calling Function ===\n",
            "Calling function: vector_tool_issue1_18308 with args: {\n",
            "  \"input\": \"contributions page\"\n",
            "}\n",
            "Got output: The contributions page is not mentioned in the given context information.\n",
            "========================\n",
            "\n",
            "STARTING TURN 2\n",
            "---------------\n",
            "\n",
            "=== Calling Function ===\n",
            "Calling function: vector_tool_issue1_18308 with args: {\n",
            "  \"input\": \"index page content\"\n",
            "}\n",
            "Got output: The given context information does not provide any information about an index page or its content. Therefore, it is not possible to answer the query based on the provided context.\n",
            "========================\n",
            "\n",
            "STARTING TURN 2\n",
            "---------------\n",
            "\n",
            "\u001b[1;3;38;2;237;90;200m[tool_issue1_18308] A: I'm sorry, but the information about the content in the index page is not available in the given context.\n",
            "\u001b[0m\u001b[1;3;38;2;90;149;237m[tool_issue1_18308] A: I'm sorry, but the information about the content of the index page is not available in the `issue1_18308.txt` part of the issue reports.\n",
            "\u001b[0mGot output: I'm sorry, but the information about the content in the contributions page and the index page is not available in the given context.\n",
            "========================\n",
            "\n",
            "STARTING TURN 2\n",
            "---------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "response = top_agent.query(\n",
        "    \"Compare the content in the contributions page vs. index page.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "cfe1dd4c-8bfd-43d0-99bc-ca60861dc418",
      "metadata": {
        "id": "cfe1dd4c-8bfd-43d0-99bc-ca60861dc418",
        "outputId": "a1d6e992-2310-4e43-ffee-a07e8d2bda13",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LlamaIndex supports two types of evaluation: LLM-based evaluation and retrieval-based evaluation. LLM-based evaluation involves evaluating the generated response using a powerful language model like GPT-4. Retrieval-based evaluation, on the other hand, involves evaluating the ranked list of items generated by the retrievers. Both types of evaluation modules are available in LlamaIndex.\n"
          ]
        }
      ],
      "source": [
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "a8d97266-8e22-43a8-adfe-b9a7f833c06d",
      "metadata": {
        "id": "a8d97266-8e22-43a8-adfe-b9a7f833c06d",
        "outputId": "7c6269e6-8b72-4ddb-cd95-200075c6de89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "STARTING TURN 1\n",
            "---------------\n",
            "\n",
            "=== Calling Function ===\n",
            "Calling function: compare_tool with args: {\n",
            "\"input\": \"Compare the tree index and list index at a high level.\"\n",
            "}\n",
            "Generated 4 sub questions.\n",
            "\u001b[1;3;38;2;237;90;200m[tool_indices_tree] Q: What is the purpose of the Tree Index?\n",
            "\u001b[0mSTARTING TURN 1\n",
            "---------------\n",
            "\n",
            "\u001b[1;3;38;2;90;149;237m[tool_indices_list] Q: What is the purpose of the List Index?\n",
            "\u001b[0mSTARTING TURN 1\n",
            "---------------\n",
            "\n",
            "\u001b[1;3;38;2;11;159;203m[tool_indices_tree] Q: What are the differences between the Tree Index and the List Index?\n",
            "\u001b[0mSTARTING TURN 1\n",
            "---------------\n",
            "\n",
            "\u001b[1;3;38;2;155;135;227m[tool_indices_list] Q: What are the differences between the Tree Index and the List Index?\n",
            "\u001b[0mSTARTING TURN 1\n",
            "---------------\n",
            "\n",
            "=== Calling Function ===\n",
            "Calling function: summary_tool_indices_tree with args: {\n",
            "  \"input\": \"purpose of Tree Index\"\n",
            "}\n",
            "=== Calling Function ===\n",
            "Calling function: vector_tool_indices_list with args: {\n",
            "  \"input\": \"Tree Index vs List Index\"\n",
            "}\n",
            "=== Calling Function ===\n",
            "Calling function: summary_tool_indices_list with args: {\n",
            "  \"input\": \"List Index\"\n",
            "}\n",
            "=== Calling Function ===\n",
            "Calling function: vector_tool_indices_tree with args: {\n",
            "  \"input\": \"differences between Tree Index and List Index\"\n",
            "}\n",
            "Got output: The context information does not provide any direct information about the difference between a Tree Index and a List Index.\n",
            "========================\n",
            "\n",
            "STARTING TURN 2\n",
            "---------------\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "BadRequestError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-d983ab3d0514>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m response = top_agent.query(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;34m\"Can you compare the tree index and list index at a very high-level?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_index/indices/query/base.py\u001b[0m in \u001b[0;36mquery\u001b[0;34m(self, str_or_query_bundle)\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr_or_query_bundle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0mstr_or_query_bundle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQueryBundle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr_or_query_bundle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr_or_query_bundle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0maquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr_or_query_bundle\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mQueryType\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mRESPONSE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_index/callbacks/utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mcallback_manager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCallbackManager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mcallback_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0masync_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_index/agent/types.py\u001b[0m in \u001b[0;36m_query\u001b[0;34m(self, query_bundle)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mtrace_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"query\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_bundle\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mQueryBundle\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mRESPONSE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         agent_response = self.chat(\n\u001b[0m\u001b[1;32m     33\u001b[0m             \u001b[0mquery_bundle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery_str\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mchat_history\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_index/callbacks/utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mcallback_manager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCallbackManager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mcallback_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0masync_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_index/agent/openai_agent.py\u001b[0m in \u001b[0;36mchat\u001b[0;34m(self, message, chat_history, tool_choice)\u001b[0m\n\u001b[1;32m    406\u001b[0m             \u001b[0mpayload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mEventPayload\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMESSAGES\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m         ) as e:\n\u001b[0;32m--> 408\u001b[0;31m             chat_response = self._chat(\n\u001b[0m\u001b[1;32m    409\u001b[0m                 \u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchat_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtool_choice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mChatResponseMode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWAIT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_index/agent/openai_agent.py\u001b[0m in \u001b[0;36m_chat\u001b[0;34m(self, message, chat_history, tool_choice, mode)\u001b[0m\n\u001b[1;32m    343\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid tool type. Unsupported by OpenAI\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m                     \u001b[0;31m# TODO: maybe execute this with multi-threading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtools\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtool_call\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m                     \u001b[0;31m# change function call to the default value, if a custom function was given\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m                     \u001b[0;31m# as an argument (none and auto are predefined by OpenAI)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_index/agent/openai_agent.py\u001b[0m in \u001b[0;36m_call_function\u001b[0;34m(self, tools, tool_call)\u001b[0m\n\u001b[1;32m    244\u001b[0m             },\n\u001b[1;32m    245\u001b[0m         ) as event:\n\u001b[0;32m--> 246\u001b[0;31m             function_message, tool_output = call_function(\n\u001b[0m\u001b[1;32m    247\u001b[0m                 \u001b[0mtools\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtool_call\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_verbose\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_index/agent/openai_agent.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(tools, tool_call, verbose)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mtool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_function_by_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtools\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0margument_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marguments_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0margument_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Got output: {output!s}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_index/tools/types.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mToolOutput\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_index/tools/query_engine.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot call query engine without inputs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_query_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         return ToolOutput(\n\u001b[1;32m     62\u001b[0m             \u001b[0mcontent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_index/indices/query/base.py\u001b[0m in \u001b[0;36mquery\u001b[0;34m(self, str_or_query_bundle)\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr_or_query_bundle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0mstr_or_query_bundle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQueryBundle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr_or_query_bundle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr_or_query_bundle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0maquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr_or_query_bundle\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mQueryType\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mRESPONSE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_index/query_engine/sub_question_query_engine.py\u001b[0m in \u001b[0;36m_query\u001b[0;34m(self, query_bundle)\u001b[0m\n\u001b[1;32m    144\u001b[0m                 ]\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m                 \u001b[0mqa_pairs_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_async_tasks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m                 \u001b[0mqa_pairs_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mSubQuestionAnswerPair\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqa_pairs_all\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_index/async_utils.py\u001b[0m in \u001b[0;36mrun_async_tasks\u001b[0;34m(tasks, show_progress, progress_bar_desc)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtasks_to_execute\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_gather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nest_asyncio.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mtask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_future\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_until_complete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nest_asyncio.py\u001b[0m in \u001b[0;36mrun_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     97\u001b[0m                 raise RuntimeError(\n\u001b[1;32m     98\u001b[0m                     'Event loop stopped before Future completed.')\n\u001b[0;32m---> 99\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/asyncio/futures.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__log_traceback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception_tb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/asyncio/tasks.py\u001b[0m in \u001b[0;36m__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    232\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_must_cancel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_index/async_utils.py\u001b[0m in \u001b[0;36m_gather\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_gather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtasks_to_execute\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_gather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/asyncio/tasks.py\u001b[0m in \u001b[0;36m__wakeup\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__wakeup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m             \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m             \u001b[0;31m# This may also be a cancellation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/asyncio/tasks.py\u001b[0m in \u001b[0;36m__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    230\u001b[0m                 \u001b[0;31m# We use the `send` method directly, because coroutines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m                 \u001b[0;31m# don't have `__iter__` and `__next__` methods.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_index/query_engine/sub_question_query_engine.py\u001b[0m in \u001b[0;36m_aquery_subq\u001b[0;34m(self, sub_q, color)\u001b[0m\n\u001b[1;32m    225\u001b[0m                     \u001b[0mprint_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[{sub_q.tool_name}] Q: {question}\\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m                 \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mquery_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m                 \u001b[0mresponse_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_index/indices/query/base.py\u001b[0m in \u001b[0;36maquery\u001b[0;34m(self, str_or_query_bundle)\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr_or_query_bundle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0mstr_or_query_bundle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQueryBundle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr_or_query_bundle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_aquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr_or_query_bundle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_bundle\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mQueryBundle\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mNodeWithScore\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_index/callbacks/utils.py\u001b[0m in \u001b[0;36masync_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mcallback_manager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCallbackManager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mcallback_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0masync_wrapper\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miscoroutinefunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_index/agent/types.py\u001b[0m in \u001b[0;36m_aquery\u001b[0;34m(self, query_bundle)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mtrace_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"query\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_aquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_bundle\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mQueryBundle\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mRESPONSE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         agent_response = await self.achat(\n\u001b[0m\u001b[1;32m     43\u001b[0m             \u001b[0mquery_bundle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery_str\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mchat_history\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_index/callbacks/utils.py\u001b[0m in \u001b[0;36masync_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mcallback_manager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCallbackManager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mcallback_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0masync_wrapper\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miscoroutinefunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_index/agent/openai_agent.py\u001b[0m in \u001b[0;36machat\u001b[0;34m(self, message, chat_history, tool_choice)\u001b[0m\n\u001b[1;32m    424\u001b[0m             \u001b[0mpayload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mEventPayload\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMESSAGES\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m         ) as e:\n\u001b[0;32m--> 426\u001b[0;31m             chat_response = await self._achat(\n\u001b[0m\u001b[1;32m    427\u001b[0m                 \u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchat_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtool_choice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mChatResponseMode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWAIT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_index/agent/openai_agent.py\u001b[0m in \u001b[0;36m_achat\u001b[0;34m(self, message, chat_history, tool_choice, mode)\u001b[0m\n\u001b[1;32m    370\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"STARTING TURN {ix}\\n---------------\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m             \u001b[0mllm_chat_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_llm_chat_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunctions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_tool_choice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m             agent_chat_response = await self._get_async_agent_response(\n\u001b[0m\u001b[1;32m    373\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mllm_chat_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_index/agent/openai_agent.py\u001b[0m in \u001b[0;36m_get_async_agent_response\u001b[0;34m(self, mode, **llm_chat_kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m     ) -> AGENT_CHAT_RESPONSE_TYPE:\n\u001b[1;32m    302\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mChatResponseMode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWAIT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m             \u001b[0mchat_response\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mChatResponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_llm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0machat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mllm_chat_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchat_response\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mChatResponseMode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTREAM\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_index/llms/base.py\u001b[0m in \u001b[0;36mwrapped_async_llm_chat\u001b[0;34m(_self, messages, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m                 )\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m                 \u001b[0mf_return_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_self\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_return_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAsyncGenerator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m                     \u001b[0;31m# intercept the generator and add a callback to the end\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_index/llms/openai.py\u001b[0m in \u001b[0;36machat\u001b[0;34m(self, messages, **kwargs)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m             \u001b[0machat_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0macompletion_to_chat_decorator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_acomplete\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0machat_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mllm_chat_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/llama_index/llms/openai.py\u001b[0m in \u001b[0;36m_achat\u001b[0;34m(self, messages, **kwargs)\u001b[0m\n\u001b[1;32m    454\u001b[0m     ) -> ChatResponse:\n\u001b[1;32m    455\u001b[0m         \u001b[0mmessage_dicts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_openai_message_dicts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m         response = await self._aclient.chat.completions.create(\n\u001b[0m\u001b[1;32m    457\u001b[0m             \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessage_dicts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_model_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m   1189\u001b[0m         \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mhttpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimeout\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mNotGiven\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNOT_GIVEN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m     ) -> ChatCompletion | AsyncStream[ChatCompletionChunk]:\n\u001b[0;32m-> 1191\u001b[0;31m         return await self._post(\n\u001b[0m\u001b[1;32m   1192\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1193\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1472\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mawait\u001b[0m \u001b[0masync_to_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1473\u001b[0m         )\n\u001b[0;32m-> 1474\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1476\u001b[0m     async def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, stream, stream_cls, remaining_retries)\u001b[0m\n\u001b[1;32m   1273\u001b[0m         \u001b[0mremaining_retries\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m     ) -> ResponseT | _AsyncStreamT:\n\u001b[0;32m-> 1275\u001b[0;31m         return await self._request(\n\u001b[0m\u001b[1;32m   1276\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, stream, stream_cls, remaining_retries)\u001b[0m\n\u001b[1;32m   1316\u001b[0m             \u001b[0;31m# to completion before attempting to access the response text.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1317\u001b[0m             \u001b[0;32mawait\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1318\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1319\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mhttpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConnectTimeout\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mretries\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': \"An assistant message with 'tool_calls' must be followed by tool messages responding to each 'tool_call_id'. The following tool_call_ids did not have response messages: call_G8t6v0XZQuqidQWo1pm4BkzP\", 'type': 'invalid_request_error', 'param': 'messages.[3].role', 'code': None}}"
          ]
        }
      ],
      "source": [
        "response = top_agent.query(\n",
        "    \"Can you compare the tree index and list index at a very high-level?\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "7401a80c-3cc7-4c72-9c45-82ffc1bd6816",
      "metadata": {
        "id": "7401a80c-3cc7-4c72-9c45-82ffc1bd6816",
        "outputId": "e8042e75-a494-457a-c932-d8c4fd8fed9a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LlamaIndex supports two types of evaluation: LLM-based evaluation and retrieval-based evaluation. LLM-based evaluation involves evaluating the generated response using a powerful language model like GPT-4. Retrieval-based evaluation, on the other hand, involves evaluating the ranked list of items generated by the retrievers. Both types of evaluation modules are available in LlamaIndex.\n"
          ]
        }
      ],
      "source": [
        "print(str(response))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = top_agent.query(\n",
        "    \"which is the best agent comparing retrieval\"\n",
        ")\n",
        "print(str(response))"
      ],
      "metadata": {
        "id": "I6ox1PQD_zpD",
        "outputId": "5fc63bbb-f0d8-4212-b2e7-af0b0377de0d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "I6ox1PQD_zpD",
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "STARTING TURN 1\n",
            "---------------\n",
            "\n",
            "=== Calling Function ===\n",
            "Calling function: compare_tool with args: {\n",
            "  \"input\": \"Which agent is the best for comparing retrieval?\"\n",
            "}\n",
            "Generated 5 sub questions.\n",
            "\u001b[1;3;38;2;237;90;200m[tool_low_level_oss_ingestion_retrieval] Q: What is the performance of Llama 2 compared to other models on retrieval benchmarks?\n",
            "\u001b[0mSTARTING TURN 1\n",
            "---------------\n",
            "\n",
            "\u001b[1;3;38;2;90;149;237m[tool_query_retrievers] Q: What are the different types of retrievers in the LLAMA Index API?\n",
            "\u001b[0mSTARTING TURN 1\n",
            "---------------\n",
            "\n",
            "\u001b[1;3;38;2;11;159;203m[tool_retrievers_list] Q: What retrievers are available in the SummaryIndex?\n",
            "\u001b[0mSTARTING TURN 1\n",
            "---------------\n",
            "\n",
            "\u001b[1;3;38;2;155;135;227m[tool_retrievers_table] Q: What retrievers are available for keyword tables?\n",
            "\u001b[0mSTARTING TURN 1\n",
            "---------------\n",
            "\n",
            "\u001b[1;3;38;2;237;90;200m[tool_low_level_retrieval] Q: How can I build a standard retriever against a vector database using Pinecone?\n",
            "\u001b[0mSTARTING TURN 1\n",
            "---------------\n",
            "\n",
            "=== Calling Function ===\n",
            "Calling function: vector_tool_retrievers_table with args: {\n",
            "  \"input\": \"keyword tables retrievers\"\n",
            "}\n",
            "=== Calling Function ===\n",
            "Calling function: vector_tool_low_level_oss_ingestion_retrieval with args: {\n",
            "  \"input\": \"performance of Llama 2 compared to other models on retrieval benchmarks\"\n",
            "}\n",
            "=== Calling Function ===\n",
            "Calling function: vector_tool_query_retrievers with args: {\n",
            "  \"input\": \"types of retrievers\"\n",
            "}\n",
            "=== Calling Function ===\n",
            "Calling function: vector_tool_retrievers_list with args: {\n",
            "  \"input\": \"SummaryIndex retrievers\"\n",
            "}\n",
            "Got output: The context information does not provide any information about the types of retrievers.\n",
            "========================\n",
            "\n",
            "STARTING TURN 2\n",
            "---------------\n",
            "\n",
            "Got output: Llama 2 outperforms all open-source models on most of the benchmarks, with an average improvement of around 5 points over the next best model (GPT-3.5).\n",
            "========================\n",
            "\n",
            "STARTING TURN 2\n",
            "---------------\n",
            "\n",
            "=== Calling Function ===\n",
            "Calling function: vector_tool_low_level_retrieval with args: {\n",
            "  \"input\": \"build a standard retriever against a vector database using Pinecone\"\n",
            "}\n",
            "Got output: There are four types of keyword table retrievers mentioned in the context information: BaseKeywordTableRetriever, KeywordTableGPTRetriever, KeywordTableRAKERetriever, and KeywordTableSimpleRetriever. These retrievers are used to extract keywords from a query. Each retriever has methods such as get_prompts(), get_service_context(), retrieve(), and update_prompts() that can be used for different purposes.\n",
            "========================\n",
            "\n",
            "STARTING TURN 2\n",
            "---------------\n",
            "\n",
            "Got output: The SummaryIndex retrievers mentioned in the context are ListIndexEmbeddingRetriever, ListIndexLLMRetriever, and ListIndexRetriever. These retrievers are used to retrieve nodes from the SummaryIndex based on a query.\n",
            "========================\n",
            "\n",
            "STARTING TURN 2\n",
            "---------------\n",
            "\n",
            "Got output: You can build a standard retriever against a vector database using Pinecone.\n",
            "========================\n",
            "\n",
            "STARTING TURN 2\n",
            "---------------\n",
            "\n",
            "=== Calling Function ===\n",
            "Calling function: summary_tool_query_retrievers with args: {\n",
            "  \"input\": \"types of retrievers\"\n",
            "}\n",
            "\u001b[1;3;38;2;11;159;203m[tool_retrievers_list] A: The retrievers available in the SummaryIndex are:\n",
            "\n",
            "1. ListIndexEmbeddingRetriever\n",
            "2. ListIndexLLMRetriever\n",
            "3. ListIndexRetriever\n",
            "\n",
            "These retrievers are used to retrieve nodes from the SummaryIndex based on a query.\n",
            "\u001b[0m\u001b[1;3;38;2;237;90;200m[tool_low_level_oss_ingestion_retrieval] A: Llama 2 demonstrates superior performance compared to other open-source models on most retrieval benchmarks. On average, it shows an improvement of around 5 points over the next best model, which is GPT-3.5.\n",
            "\u001b[0mGot output: The types of retrievers mentioned in the given information include:\n",
            "\n",
            "1. Index Retrievers: These are retriever classes specific to the index. Some examples mentioned are Empty Index Retriever, Knowledge Graph Retriever, List Retriever, Keyword Table Retrievers, Tree Retrievers, and Vector Store Retrievers.\n",
            "\n",
            "2. Additional Retrievers: These are additional retriever classes that can augment existing retrievers with new capabilities. An example mentioned is the Transform Retriever.\n",
            "\n",
            "3. Base Retriever: This is the base retriever class that contains the retrieve method shared among all retrievers. It also has methods like get_prompts(), get_service_context(), and update_prompts().\n",
            "\n",
            "Please note that the given information does not provide an exhaustive list of all retrievers, but it does mention some specific types of retrievers.\n",
            "========================\n",
            "\n",
            "STARTING TURN 3\n",
            "---------------\n",
            "\n",
            "\u001b[1;3;38;2;155;135;227m[tool_retrievers_table] A: The available retrievers for keyword tables are:\n",
            "\n",
            "1. BaseKeywordTableRetriever\n",
            "2. KeywordTableGPTRetriever\n",
            "3. KeywordTableRAKERetriever\n",
            "4. KeywordTableSimpleRetriever\n",
            "\n",
            "These retrievers are used to extract keywords from a query and each has methods such as `get_prompts()`, `get_service_context()`, `retrieve()`, and `update_prompts()` for different purposes.\n",
            "\u001b[0m\u001b[1;3;38;2;237;90;200m[tool_low_level_retrieval] A: To build a standard retriever against a vector database using Pinecone, you need to follow these steps:\n",
            "\n",
            "1. First, you need to create a Pinecone Indexer. This is done by initializing a Pinecone Indexer object and specifying the desired vector dimension and metric type.\n",
            "\n",
            "2. Next, you need to add vectors to the Pinecone Indexer. This is done by calling the `upsert` method on the Pinecone Indexer object and passing in a dictionary where the keys are unique identifiers for the vectors and the values are the vectors themselves.\n",
            "\n",
            "3. After adding vectors, you can create a Pinecone Retriever. This is done by initializing a Pinecone Retriever object and specifying the Pinecone Indexer object as the indexer.\n",
            "\n",
            "4. Finally, you can use the Pinecone Retriever to retrieve vectors from the Pinecone Indexer. This is done by calling the `query` method on the Pinecone Retriever object and passing in a query vector. The method returns a list of the identifiers of the most similar vectors in the Pinecone Indexer.\n",
            "\n",
            "Please note that the exact code and parameters may vary depending on your specific use case and the version of Pinecone you are using.\n",
            "\u001b[0m\u001b[1;3;38;2;90;149;237m[tool_query_retrievers] A: The LLAMA Index API mentions several types of retrievers:\n",
            "\n",
            "1. **Index Retrievers**: These are specific to the index and include the Empty Index Retriever, Knowledge Graph Retriever, List Retriever, Keyword Table Retrievers, Tree Retrievers, and Vector Store Retrievers.\n",
            "\n",
            "2. **Additional Retrievers**: These augment existing retrievers with new capabilities. An example is the Transform Retriever.\n",
            "\n",
            "3. **Base Retriever**: This is the base class that contains the retrieve method shared among all retrievers. It also has methods like get_prompts(), get_service_context(), and update_prompts().\n",
            "\n",
            "Please note that this is not an exhaustive list of all retrievers, but it does mention some specific types.\n",
            "\u001b[0mGot output: Llama 2 is the best agent for comparing retrieval. It demonstrates superior performance compared to other open-source models on most retrieval benchmarks, showing an improvement of around 5 points over the next best model, GPT-3.5.\n",
            "========================\n",
            "\n",
            "STARTING TURN 2\n",
            "---------------\n",
            "\n",
            "The best agent for comparing retrieval is Llama 2. It outperforms other models, including GPT-3.5, on various benchmarks, with an average improvement of around 5 points.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "llama_index_v2",
      "language": "python",
      "name": "llama_index_v2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3"
    },
    "colab": {
      "provenance": []
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e165d21342764c27be24919690efb684": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_148af4a51c964e12b954e874020ce652",
              "IPY_MODEL_c35ef3b257904bbd81cd1b72b637738f",
              "IPY_MODEL_c2db72fed34a4cf4a57b242c9a34a738"
            ],
            "layout": "IPY_MODEL_330c23ea42e54e96b795c7358f0df652"
          }
        },
        "148af4a51c964e12b954e874020ce652": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_39e1da326e4743369d6d07ffb840b1e7",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_ad576ad0749e4606bc64fa4547751240",
            "value": "100%"
          }
        },
        "c35ef3b257904bbd81cd1b72b637738f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_689225f033f841aab98dc6f9ac61ec2d",
            "max": 15,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3eb88aed177e43de9b7d5c965713e0bf",
            "value": 15
          }
        },
        "c2db72fed34a4cf4a57b242c9a34a738": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e59dbb0078eb4f4098d583e1bfa0a79d",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_b84a3bdf4bb34351b7f4048c33f09307",
            "value": " 15/15 [00:04&lt;00:00,  4.73it/s]"
          }
        },
        "330c23ea42e54e96b795c7358f0df652": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39e1da326e4743369d6d07ffb840b1e7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ad576ad0749e4606bc64fa4547751240": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "689225f033f841aab98dc6f9ac61ec2d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3eb88aed177e43de9b7d5c965713e0bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e59dbb0078eb4f4098d583e1bfa0a79d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b84a3bdf4bb34351b7f4048c33f09307": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}